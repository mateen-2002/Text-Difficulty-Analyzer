{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilhgh9ES7G1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np                                                              #Used for numerical computations \n",
        "import pandas as pd                                                             #Used for reading the data\n",
        "import matplotlib.pyplot as plt                                                 #Used for plotting \n",
        "from nltk.corpus import stopwords                                               #This is used to plot the number of stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize                          #This is used to divide the overall text data to tokens and sentences\n",
        "import tqdm                                                                     #Used for measuring the time it takes to get the things done \n",
        "import re                                                                       #Standard library for reading and substituting the word expressions \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tqdm import tqdm\n",
        "import nltk                                                                     #Used for the natural language processing tasks \n",
        "from wordcloud import WordCloud                                                 #It is used to plot the frequency of the words which determines their size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoWP3dIoS00y",
        "outputId": "03d31b4b-4bf0-491d-da91-79d183f85443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1inht9RAihRP"
      },
      "outputs": [],
      "source": [
        "##Reading the training data, testing data and sample values that we are going to be understanding and using in the long term. \n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/mp2_dataset/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/mp2_dataset/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVqifuPKjSHh",
        "outputId": "03ea0969-3b20-42d9-dd47-e63ba965b238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "'''\n",
        "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.\n",
        "You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more.\n",
        "'''\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "'''\n",
        "Punkt Sentence Tokenizer\n",
        "\n",
        "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n",
        "It must be trained on a large collection of plaintext in the target language before it can be used.\n",
        "\n",
        "The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
        "'''\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('omw-1.4')        #Open Multilingual Wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnbnl2mA7dop"
      },
      "outputs": [],
      "source": [
        "def preprocessing_function(df):\n",
        "    \"\"\"\n",
        "    This function takes into consideration the dataframe and extracts the text.\n",
        "    In addition, it makes modifications to the text and converts it to a simpler form\n",
        "    for machine learning processing respectively.\"\"\"\n",
        "    \n",
        "    text_list = []\n",
        "    for text in tqdm(df['excerpt'].values):\n",
        "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "        text = text.lower()\n",
        "        text = nltk.word_tokenize(text)\n",
        "        [word for word in text if not word in set(stopwords.words(\"english\"))]\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        text = [lemmatizer.lemmatize(word) for word in text]\n",
        "        text = \" \".join(text)\n",
        "        text_list.append(text)\n",
        "    text_list = pd.Series(text_list)\n",
        "    text_list.column = ['Converted_text']\n",
        "    return text_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nck4Xd49gTJD",
        "outputId": "f07c9d20-a6df-4e16-dfff-600eb2548a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2834/2834 [01:07<00:00, 41.90it/s]\n",
            "100%|██████████| 7/7 [00:00<00:00, 47.80it/s]\n"
          ]
        }
      ],
      "source": [
        "preprocessed_text = preprocessing_function(df_train)\n",
        "preprocessed_text_test = preprocessing_function(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nbo0sFJgAMN",
        "outputId": "334655be-e268-43cd-c834-1dfcf5251b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       when the young people returned to the ballroom...\n",
            "1       all through dinner time mr fayre wa somewhat s...\n",
            "2       a roger had predicted the snow departed a quic...\n",
            "3       and outside before the palace a great garden w...\n",
            "4       once upon a time there were three bear who liv...\n",
            "                              ...                        \n",
            "2829    when you think of dinosaur and where they live...\n",
            "2830    so what is a solid solid are usually hard beca...\n",
            "2831    the second state of matter we will discus is a...\n",
            "2832    solid are shape that you can actually touch th...\n",
            "2833    animal are made of many cell they eat thing an...\n",
            "Length: 2834, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(preprocessed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqkHwWrrkPjq"
      },
      "source": [
        "**Defining get_useful_features functions**\n",
        "\n",
        "\n",
        "It is now time to get the useful features that are important for machine learning. We would have to be creating new features that would help the machine learning models to get the best predictions for the difficulty of the text.\n",
        "\n",
        "Taking into consideration the excerpt and stopwords, we are going to be creating new feautres such as total number of words, sentence length, overall change in the text length and other features that are important for getting the machine learning outputs. The function would return the final dataframe that contains all the preprocessed output along with the newly created features that are important for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylUy-TMfO79v"
      },
      "outputs": [],
      "source": [
        "def get_useful_features(df, stop_words):\n",
        "    \"\"\"\n",
        "    The function would take the dataframe and stopwords and then, convert the excerpts into different features\n",
        "    such as the number of sentences, words and the lenght of the lemmas created along with the overall preprocessed\n",
        "    essay length.\"\"\"\n",
        "    sentences = []\n",
        "    num_of_words = []\n",
        "    sent_length = []\n",
        "    word_length = []\n",
        "    lemma_length = []\n",
        "    num_of_lemmas = []\n",
        "    preprocessed_essay_length = []\n",
        "    initial_text_length = []\n",
        "    num_of_sentences = []\n",
        "    text_shortage = []\n",
        "    \n",
        "    for text in tqdm(df['excerpt'].values):\n",
        "        \n",
        "        initial_length = len(text)\n",
        "        initial_text_length.append(initial_length)\n",
        "        num_sentences = len(sent_tokenize(text))\n",
        "        num_of_sentences.append(num_sentences)\n",
        "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "        text = text.lower()\n",
        "        text = word_tokenize(text)\n",
        "        num_words = len(text) \n",
        "        num_of_words.append(num_words)\n",
        "        sent_length.append(num_words/num_sentences)\n",
        "        word_length.append(initial_length/num_words)\n",
        "        text = [word for word in text if not word in stop_words]\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        text = [lemmatizer.lemmatize(word) for word in text]\n",
        "        #print(text)\n",
        "        num_lemmas = len(text)\n",
        "        num_of_lemmas.append(num_lemmas)\n",
        "        text = \" \".join(text)\n",
        "        #print(text)\n",
        "        preprocessed_essay_length_value = len(text)\n",
        "        preprocessed_essay_length.append(preprocessed_essay_length_value)\n",
        "        #print(preprocessed_essay_length)\n",
        "        #print(num_lemmas)\n",
        "        lemma_length.append(preprocessed_essay_length_value/num_lemmas)\n",
        "        \n",
        "        text_shortage.append(preprocessed_essay_length_value/initial_length)\n",
        "        \n",
        "    final_df = pd.concat([pd.Series(sent_length), pd.Series(num_of_words),\n",
        "                             pd.Series(word_length), pd.Series(lemma_length),\n",
        "                             pd.Series(num_of_sentences), pd.Series(initial_text_length),\n",
        "                             pd.Series(num_of_lemmas), pd.Series(preprocessed_essay_length),\n",
        "                             pd.Series(text_shortage)], axis = 1)\n",
        "    final_df.columns = [\"sentence_length\", \"num_of_words\", \"word_length\",\n",
        "                           \"lemma_length\", \"num_of_sentences\",\n",
        "                           \"initial_text_length\", \"num_of_lemmas\",\n",
        "                           \"preprocessed_essay_length\", \"text_shortage\"]\n",
        "    \n",
        "    return final_df\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8dlarxtklqC",
        "outputId": "5940ccff-9a2a-4db1-9c5e-4152235fdbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2834/2834 [00:05<00:00, 560.27it/s]\n"
          ]
        }
      ],
      "source": [
        "final_df = get_useful_features(df_train, stop_words = set(stopwords.words(\"english\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1Zyn3eXkvcG",
        "outputId": "6ce0b4f0-ffb4-4c97-af43-9ad812927be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 495.16it/s]\n"
          ]
        }
      ],
      "source": [
        "final_df_test = get_useful_features(df_test, stop_words = set(stopwords.words(\"english\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9FHaaHR-xjC",
        "outputId": "3308b489-4d27-4352-b7b0-ac0fec904165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentence_length  num_of_words  word_length  lemma_length  num_of_sentences  \\\n",
            "0        16.454545           181     5.480663      6.494505                11   \n",
            "1        11.466667           172     5.447674      6.482353                15   \n",
            "2        15.636364           172     5.279070      6.273810                11   \n",
            "3        33.400000           167     5.443114      6.095745                 5   \n",
            "4        30.200000           151     4.788079      5.581081                 5   \n",
            "\n",
            "   initial_text_length  num_of_lemmas  preprocessed_essay_length  \\\n",
            "0                  992             91                        591   \n",
            "1                  937             85                        551   \n",
            "2                  908             84                        527   \n",
            "3                  909             94                        573   \n",
            "4                  723             74                        413   \n",
            "\n",
            "   text_shortage  \n",
            "0       0.595766  \n",
            "1       0.588047  \n",
            "2       0.580396  \n",
            "3       0.630363  \n",
            "4       0.571231  \n"
          ]
        }
      ],
      "source": [
        "print(final_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0i3GtfvlFaN"
      },
      "source": [
        "## 3.7 Creating new function that generates more features\n",
        "\n",
        "We are going to create a function that would create more features such as counting the number of commas, semicolons and other important features that are important for machine learning analysis. We would have to create empty lists of these values and we are going to concat those by converting them into series and then, return a new dataframe respectively. \n",
        "\n",
        "With the help of this function, we have created new dataframe which contains the useful columns that are important for machine learning respectively. We would be performing the feature analysis and thise ensures that we are going to be getting the best results on the test set respectively. \n",
        "\n",
        "At last, we are going to concat those values that are important for machine learning and this would ensure that we get the best results in the test set respectively. We are going to be taking those values and this ensures that we are getting the best results on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVIcZZRzO790"
      },
      "outputs": [],
      "source": [
        "def generate_more_features(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    This function would create a dataframe of different useful features\n",
        "    that are important for machine learning predictions respectively.\n",
        "    \"\"\"\n",
        "    commas = []\n",
        "    semicolon = []\n",
        "    exclamations = []\n",
        "    questions = []\n",
        "    quotes = []\n",
        "    periods = []\n",
        "    longest_word = []\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        \n",
        "        #word_len = []\n",
        "        text = df['excerpt'].iloc[i]\n",
        "        commas.append(text.count(\",\"))\n",
        "        semicolon.append(text.count(\";\"))\n",
        "        exclamations.append(text.count(\"!\"))\n",
        "        questions.append(text.count(\"?\"))\n",
        "        quotes.append(text.count('\"'))\n",
        "        periods.append(text.count('.'))\n",
        "        word_len = [len(w) for w in text.split(\" \")]\n",
        "        longest_word.append(np.max(word_len))\n",
        "        \n",
        "    df_with_features =pd.concat((pd.Series(commas), pd.Series(semicolon), pd.Series(exclamations),\n",
        "                               pd.Series(questions), pd.Series(quotes), pd.Series(periods),\n",
        "                                pd.Series(longest_word)), axis = 1)\n",
        "    df_with_features.columns = [\"num_of_commas\", \"num_of_semicolons\", \"num_of_explamations\",\n",
        "                                \"num_of_questions\", \"num_of_quotes\", \"num_of_periods\", \n",
        "                                \"longest_word\"]\n",
        "                                \n",
        "    return df_with_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk-Un42klPey"
      },
      "outputs": [],
      "source": [
        "df_with_more_features = generate_more_features(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuXc-JEYlVnU"
      },
      "outputs": [],
      "source": [
        "df_with_more_features_test = generate_more_features(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1gRu5fPBJe_",
        "outputId": "3a07eac7-af3a-401d-89ce-9842e3f38e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   num_of_commas  num_of_semicolons  num_of_explamations  num_of_questions  \\\n",
            "0             14                  0                    0                 0   \n",
            "1             24                  0                    5                 2   \n",
            "2             17                  2                    1                 0   \n",
            "3             23                  2                    0                 0   \n",
            "4             13                 10                    0                 0   \n",
            "\n",
            "   num_of_quotes  num_of_periods  longest_word  \n",
            "0              0              11            14  \n",
            "1             12              10            15  \n",
            "2             10              11            14  \n",
            "3              0               5            13  \n",
            "4              0               5            12  \n"
          ]
        }
      ],
      "source": [
        "print(df_with_more_features.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUHyjHvFhTUJ"
      },
      "outputs": [],
      "source": [
        "df_complete = pd.concat((preprocessed_text, df_with_more_features, final_df), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKp95IfIhZve"
      },
      "outputs": [],
      "source": [
        "df_complete_test = pd.concat((preprocessed_text_test, df_with_more_features_test, final_df_test), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1MRU96jk4N4",
        "outputId": "d1b5b4d0-bc03-42a7-9d01-d2425f1dd3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                      0  num_of_commas  \\\n",
            "0     when the young people returned to the ballroom...             14   \n",
            "1     all through dinner time mr fayre wa somewhat s...             24   \n",
            "2     a roger had predicted the snow departed a quic...             17   \n",
            "3     and outside before the palace a great garden w...             23   \n",
            "4     once upon a time there were three bear who liv...             13   \n",
            "...                                                 ...            ...   \n",
            "2829  when you think of dinosaur and where they live...             12   \n",
            "2830  so what is a solid solid are usually hard beca...              5   \n",
            "2831  the second state of matter we will discus is a...              2   \n",
            "2832  solid are shape that you can actually touch th...              8   \n",
            "2833  animal are made of many cell they eat thing an...             23   \n",
            "\n",
            "      num_of_semicolons  num_of_explamations  num_of_questions  num_of_quotes  \\\n",
            "0                     0                    0                 0              0   \n",
            "1                     0                    5                 2             12   \n",
            "2                     2                    1                 0             10   \n",
            "3                     2                    0                 0              0   \n",
            "4                    10                    0                 0              0   \n",
            "...                 ...                  ...               ...            ...   \n",
            "2829                  0                    0                 3              0   \n",
            "2830                  0                    0                 1              0   \n",
            "2831                  1                    0                 1              0   \n",
            "2832                  1                    0                 0              0   \n",
            "2833                  1                    0                 0              0   \n",
            "\n",
            "      num_of_periods  longest_word  sentence_length  num_of_words  \\\n",
            "0                 11            14        16.454545           181   \n",
            "1                 10            15        11.466667           172   \n",
            "2                 11            14        15.636364           172   \n",
            "3                  5            13        33.400000           167   \n",
            "4                  5            12        30.200000           151   \n",
            "...              ...           ...              ...           ...   \n",
            "2829              10            15        11.153846           145   \n",
            "2830              13            15        11.714286           164   \n",
            "2831              16            14        11.352941           193   \n",
            "2832              12            28        13.083333           157   \n",
            "2833              13            16        11.153846           145   \n",
            "\n",
            "      word_length  lemma_length  num_of_sentences  initial_text_length  \\\n",
            "0        5.480663      6.494505                11                  992   \n",
            "1        5.447674      6.482353                15                  937   \n",
            "2        5.279070      6.273810                11                  908   \n",
            "3        5.443114      6.095745                 5                  909   \n",
            "4        4.788079      5.581081                 5                  723   \n",
            "...           ...           ...               ...                  ...   \n",
            "2829     6.213793      6.988372                13                  901   \n",
            "2830     5.426829      6.469136                14                  890   \n",
            "2831     4.917098      6.278481                17                  949   \n",
            "2832     5.503185      6.387500                12                  864   \n",
            "2833     6.103448      6.922078                13                  885   \n",
            "\n",
            "      num_of_lemmas  preprocessed_essay_length  text_shortage  \n",
            "0                91                        591       0.595766  \n",
            "1                85                        551       0.588047  \n",
            "2                84                        527       0.580396  \n",
            "3                94                        573       0.630363  \n",
            "4                74                        413       0.571231  \n",
            "...             ...                        ...            ...  \n",
            "2829             86                        601       0.667037  \n",
            "2830             81                        524       0.588764  \n",
            "2831             79                        496       0.522655  \n",
            "2832             80                        511       0.591435  \n",
            "2833             77                        533       0.602260  \n",
            "\n",
            "[2834 rows x 17 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df_complete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL3hDPYChduM"
      },
      "outputs": [],
      "source": [
        "df_complete_important_features = df_complete.iloc[:, 1:]\n",
        "df_complete_important_features_test = df_complete_test.iloc[:, 1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh_DPFtUacn4",
        "outputId": "b0b044dd-0a82-45a5-e290-249e15693709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      num_of_commas  num_of_semicolons  num_of_explamations  num_of_questions  \\\n",
            "0                14                  0                    0                 0   \n",
            "1                24                  0                    5                 2   \n",
            "2                17                  2                    1                 0   \n",
            "3                23                  2                    0                 0   \n",
            "4                13                 10                    0                 0   \n",
            "...             ...                ...                  ...               ...   \n",
            "2829             12                  0                    0                 3   \n",
            "2830              5                  0                    0                 1   \n",
            "2831              2                  1                    0                 1   \n",
            "2832              8                  1                    0                 0   \n",
            "2833             23                  1                    0                 0   \n",
            "\n",
            "      num_of_quotes  num_of_periods  longest_word  sentence_length  \\\n",
            "0                 0              11            14        16.454545   \n",
            "1                12              10            15        11.466667   \n",
            "2                10              11            14        15.636364   \n",
            "3                 0               5            13        33.400000   \n",
            "4                 0               5            12        30.200000   \n",
            "...             ...             ...           ...              ...   \n",
            "2829              0              10            15        11.153846   \n",
            "2830              0              13            15        11.714286   \n",
            "2831              0              16            14        11.352941   \n",
            "2832              0              12            28        13.083333   \n",
            "2833              0              13            16        11.153846   \n",
            "\n",
            "      num_of_words  word_length  lemma_length  num_of_sentences  \\\n",
            "0              181     5.480663      6.494505                11   \n",
            "1              172     5.447674      6.482353                15   \n",
            "2              172     5.279070      6.273810                11   \n",
            "3              167     5.443114      6.095745                 5   \n",
            "4              151     4.788079      5.581081                 5   \n",
            "...            ...          ...           ...               ...   \n",
            "2829           145     6.213793      6.988372                13   \n",
            "2830           164     5.426829      6.469136                14   \n",
            "2831           193     4.917098      6.278481                17   \n",
            "2832           157     5.503185      6.387500                12   \n",
            "2833           145     6.103448      6.922078                13   \n",
            "\n",
            "      initial_text_length  num_of_lemmas  preprocessed_essay_length  \\\n",
            "0                     992             91                        591   \n",
            "1                     937             85                        551   \n",
            "2                     908             84                        527   \n",
            "3                     909             94                        573   \n",
            "4                     723             74                        413   \n",
            "...                   ...            ...                        ...   \n",
            "2829                  901             86                        601   \n",
            "2830                  890             81                        524   \n",
            "2831                  949             79                        496   \n",
            "2832                  864             80                        511   \n",
            "2833                  885             77                        533   \n",
            "\n",
            "      text_shortage  \n",
            "0          0.595766  \n",
            "1          0.588047  \n",
            "2          0.580396  \n",
            "3          0.630363  \n",
            "4          0.571231  \n",
            "...             ...  \n",
            "2829       0.667037  \n",
            "2830       0.588764  \n",
            "2831       0.522655  \n",
            "2832       0.591435  \n",
            "2833       0.602260  \n",
            "\n",
            "[2834 rows x 16 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df_complete_important_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1o2Q_B8rpj-"
      },
      "source": [
        "**Feature Scaling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcoxKDxEhqsX"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC69pw0Thr_k"
      },
      "outputs": [],
      "source": [
        "def min_max_scaler(df_train, df_test):\n",
        "    \"\"\"\n",
        "    This function performs the scaling operation by taking into account the train and test set respectively. It is going to \n",
        "    convert the values that are present in the data to lie between 0 and 1 respectively.\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(df_train)\n",
        "    df_scaled = scaler.transform(df_train)\n",
        "    df_scaled_test = scaler.transform(df_test)\n",
        "    return df_scaled, df_scaled_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE6Zup31hvDD"
      },
      "outputs": [],
      "source": [
        "df_scaled, df_scaled_test = min_max_scaler(df_complete_important_features, df_complete_important_features_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc1ClHZwObhY"
      },
      "outputs": [],
      "source": [
        "#Splitting the dataset into training and testing sets\n",
        "x = df_scaled\n",
        "y = df_train['target'].values\n",
        "x_train, x_cv, y_train, y_cv = train_test_split(x, y, test_size = 0.3, random_state = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jT9y0K6r4CH"
      },
      "source": [
        "**Training our dataset over various models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpIjBk2tR-qd",
        "outputId": "8c4f7e3c-1d7d-4a4d-e08f-7ea2d0299a8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDRegressor(random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#SGDRegreesor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "sgd_reg = SGDRegressor(random_state=42)\n",
        "sgd_reg.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uaWK0yrVbWf",
        "outputId": "cce70ced-8ae4-4e8f-f68c-1b8f1bab9873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7153011475355732\n"
          ]
        }
      ],
      "source": [
        "#MSE of SGDRegreesor\n",
        "y_pred = sgd_reg.predict(x_cv)\n",
        "print(mean_squared_error(y_cv,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKMxMfdKVrSk",
        "outputId": "61608e4d-05c8-47e8-e5f3-b0d577df4fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7153011475355732\n"
          ]
        }
      ],
      "source": [
        "#RandomForestRegressor and it's MSE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rnd_reg = RandomForestRegressor(n_estimators=100,random_state=42)\n",
        "rnd_reg.fit(x_train,y_train)\n",
        "y_pred = sgd_reg.predict(x_cv)\n",
        "print(mean_squared_error(y_cv,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppyjQ2JFPIH3",
        "outputId": "91c1cce8-15a2-4de4-e697-0658e9000e3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "#GradientBoostingRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXYkqJYzPgC6",
        "outputId": "7a708086-b480-46e7-9802-0a906bb6c4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual value -0.06499875\n",
            "Predicted value [-0.75157599]\n"
          ]
        }
      ],
      "source": [
        "#Prediction\n",
        "print(\"Actual value\",y_cv[9])\n",
        "print(\"Predicted value\",model.predict([x_cv[9,:]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv_7XPZ_Qr92",
        "outputId": "e9b37b47-7f72-4bd2-c302-7b5004a26e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6415022648606243\n"
          ]
        }
      ],
      "source": [
        "#MSE of GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = model.predict(x_cv)\n",
        "print(mean_squared_error(y_cv,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-8UByWbU3E1",
        "outputId": "5f99e272-3649-4602-fecd-95d11b3f3d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (1.1.4)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15982 sha256=1ef2114f25214aeaeb74482d2b137f1151c6b3bfaa9770d70a1badebab45e1c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2022.9.24)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask\n",
        "!pip install pyngrok==4.1.1\n",
        "!pip install flask_ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dqAp5Ul-gYU"
      },
      "outputs": [],
      "source": [
        "from flask import Flask,request,render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfYFcavhq_qK",
        "outputId": "a3b5f6a2-084a-48e6-83d7-32fc1078b51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://aff3-34-125-193-93.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [24/Nov/2022 01:59:11] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Nov/2022 01:59:11] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "100%|██████████| 1/1 [00:00<00:00, 245.83it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 2686.93it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [24/Nov/2022 02:00:21] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2HFX6zbW3w5q5cZv6703lG5u1HL_3fVS7PL6xxme3ECU4L2qa\n",
        "app = Flask(__name__,template_folder='/content/drive/MyDrive/mp2_dataset/')\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "  return render_template(\"mp.html\")\n",
        "@app.route('/predict',methods=['POST','GET'])\n",
        "def predict():\n",
        "  if request.method == 'POST':\n",
        "    paragraph = request.form['iptext1']\n",
        "  df = {\"excerpt\":[paragraph]}\n",
        "  df = pd.DataFrame(df)\n",
        "  def solution(df):\n",
        "    preprocessed_text = preprocessing_function(df)\n",
        "    final_df = get_useful_features(df, stop_words = set(stopwords.words(\"english\")))\n",
        "    df_with_more_features = generate_more_features(df)\n",
        "    df_complete = pd.concat((preprocessed_text, df_with_more_features, final_df), axis = 1)\n",
        "    df_complete_important_features = df_complete.iloc[:, 1:]\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(df_complete_important_features)\n",
        "    df_scaled = scaler.transform(df_complete_important_features)\n",
        "    return model.predict(df_scaled)\n",
        "  result = solution(df)\n",
        "  if result >= 0.3:\n",
        "    res = \"EASY\"\n",
        "  elif result >= -1 and result < 1: \n",
        "     res = \"EASY\"\n",
        "  else:\n",
        "     res = \"HARD\"\n",
        "  return render_template('mp.html',result=res)\n",
        "  \n",
        "\n",
        "app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSeAlakMeKMe",
        "outputId": "71052544-2d9b-4e9a-f287-7b2f7484f8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sun is a huge ball of gases. It has a diameter of 1,392,000 km. It is so huge that it can hold millions of planets inside it.\n"
          ]
        }
      ],
      "source": [
        "#Test\n",
        "\n",
        "paragraph = input()\n",
        "data = {\"excerpt\":[paragraph]}\n",
        "df = pd.DataFrame(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv1Mp0tofg9F"
      },
      "outputs": [],
      "source": [
        " def solution(df):\n",
        "    preprocessed_text = preprocessing_function(df)\n",
        "    print( preprocessed_text)\n",
        "    final_df = get_useful_features(df, stop_words = set(stopwords.words(\"english\")))\n",
        "    df_with_more_features = generate_more_features(df)\n",
        "    df_complete = pd.concat((preprocessed_text, df_with_more_features, final_df), axis = 1)\n",
        "    df_complete_important_features = df_complete.iloc[:, 1:]\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(df_complete_important_features)\n",
        "    df_scaled = scaler.transform(df_complete_important_features)\n",
        "    return model.predict(df_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O89ue9SEfqSo",
        "outputId": "5bc63691-0e29-46ea-86a9-ebafeb62326d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 113.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    the sun is a huge ball of gas it ha a diameter...\n",
            "dtype: object\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1004.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODERATE\n",
            "[0.0801772]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "result = solution(df)\n",
        "if result >= 1:\n",
        "  res = \"EASY\"\n",
        "elif result >= -1 and result < 1: \n",
        "    res = \"MODERATE\"\n",
        "else:\n",
        "    res = \"HARD\"\n",
        "print(res)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}